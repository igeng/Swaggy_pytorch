{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_unit:  torch.Size([2, 16, 5, 5])\n",
      "lenet_unit:  torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Lenet5(nn.Module):\n",
    "    \"\"\"\n",
    "    for cifar10 dataset\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Lenet5, self).__init__()\n",
    "\n",
    "        self.conv_unit = nn.Sequential(\n",
    "            #x:[batchsz, 3, 32, 32]==>[b, 6, 20, 20]\n",
    "            nn.Conv2d(3, 6, kernel_size=5, stride=1, padding=0),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            #\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            #\n",
    "\n",
    "        )\n",
    "        # flatten\n",
    "        # fc_unit\n",
    "        self.fc_unit = nn.Sequential(\n",
    "            nn.Linear(16*5*5, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "\n",
    "        # [b, 3, 32, 32]\n",
    "        tmp = torch.randn(2, 3, 32, 32)\n",
    "        out = self.conv_unit(tmp)\n",
    "        # [b, 16, 5, 5]\n",
    "        print(\"conv_unit: \",out.shape)\n",
    "\n",
    "        #use Cross Entropy Loss,contain softmax\n",
    "        # self.criteon = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: [b, 3, 32, 32]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        batchsz = x.size(0)\n",
    "        # [b, 3, 32, 32]=>[b, 16, 5, 5]\n",
    "        x = self.conv_unit(x)\n",
    "        # [b, 16, 5, 5]=>[b, 16*5*5]\n",
    "        x = x.view(batchsz, 16*5*5)\n",
    "        # x = x.view(batchsz, -1)\n",
    "        # [b, 16*5*5] => [b, 10]\n",
    "        logits = self.fc_unit(x)\n",
    "\n",
    "        # [b, 10]\n",
    "        # pred = F.softmax(logits, dim=1)\n",
    "        # loss = self.criteon(logits, y)\n",
    "        return  logits\n",
    "\n",
    "def main():\n",
    "\n",
    "    net = Lenet5()\n",
    "\n",
    "    tmp = torch.randn(2, 3, 32, 32)\n",
    "    out = net(tmp)\n",
    "    # [b, 16, 5, 5]\n",
    "    print(\"lenet_unit: \", out.shape)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "x: torch.Size([128, 3, 32, 32]) label: torch.Size([128])\n",
      "ResNet18(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(3, 3))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (blk1): ResBlk(\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (extra): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (blk2): ResBlk(\n",
      "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (extra): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (blk3): ResBlk(\n",
      "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (extra): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (blk4): ResBlk(\n",
      "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (extra): Sequential()\n",
      "  )\n",
      "  (outlayer): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 39403520/170498071 [00:50<01:50, 1186025.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 1.0036799907684326\n",
      "0 0.5901\n",
      "1 loss: 0.7609409093856812\n",
      "1 0.6118\n",
      "2 loss: 0.7094250321388245\n",
      "2 0.6885\n",
      "3 loss: 0.47448891401290894\n",
      "3 0.6217\n",
      "4 loss: 0.4469822943210602\n",
      "4 0.6546\n",
      "5 loss: 0.23568415641784668\n",
      "5 0.6957\n",
      "6 loss: 0.47187119722366333\n",
      "6 0.6813\n",
      "7 loss: 0.23529155552387238\n",
      "7 0.6801\n",
      "8 loss: 0.16182735562324524\n",
      "8 0.695\n",
      "9 loss: 0.054022323340177536\n",
      "9 0.6662\n",
      "10 loss: 0.3160094618797302\n",
      "10 0.6849\n",
      "11 loss: 0.16224835813045502\n",
      "11 0.693\n",
      "12 loss: 0.07230500876903534\n",
      "12 0.6763\n",
      "13 loss: 0.13372336328029633\n",
      "13 0.6494\n",
      "14 loss: 0.25781184434890747\n",
      "14 0.6845\n",
      "15 loss: 0.04835467040538788\n",
      "15 0.6927\n",
      "16 loss: 0.0281381793320179\n",
      "16 0.7055\n",
      "17 loss: 0.10492197424173355\n",
      "17 0.6841\n",
      "18 loss: 0.05755515769124031\n",
      "18 0.6859\n",
      "19 loss: 0.12506401538848877\n",
      "19 0.676\n",
      "20 loss: 0.13523361086845398\n",
      "20 0.684\n",
      "21 loss: 0.021918747574090958\n",
      "21 0.7001\n",
      "22 loss: 0.05151817947626114\n",
      "22 0.6933\n",
      "23 loss: 0.07118099927902222\n",
      "23 0.6761\n",
      "24 loss: 0.03691953048110008\n",
      "24 0.6989\n",
      "25 loss: 0.07858820259571075\n",
      "25 0.6933\n",
      "26 loss: 0.05588197708129883\n",
      "26 0.6936\n",
      "27 loss: 0.0166879091411829\n",
      "27 0.6805\n",
      "28 loss: 0.0033916146494448185\n",
      "28 0.7047\n",
      "29 loss: 0.030367691069841385\n",
      "29 0.6899\n",
      "30 loss: 0.03864641115069389\n",
      "30 0.7053\n",
      "31 loss: 0.18358086049556732\n",
      "31 0.7024\n",
      "32 loss: 0.01694200001657009\n",
      "32 0.6786\n",
      "33 loss: 0.02149060182273388\n",
      "33 0.6993\n",
      "34 loss: 0.0029766708612442017\n",
      "34 0.6887\n",
      "35 loss: 0.08561459183692932\n",
      "35 0.6989\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "# from dragon_dl.dragon_pytorch.lesson45.resnet import ResNet18\n",
    "\n",
    "def main():\n",
    "    batchsz = 128\n",
    "\n",
    "    cifar_train = datasets.CIFAR10('data', True, transform=transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor()\n",
    "    ]), download=True)\n",
    "    cifar_train = DataLoader(cifar_train, batch_size=batchsz, shuffle=True)\n",
    "\n",
    "    cifar_test = datasets.CIFAR10('data', False, transform=transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor()\n",
    "    ]), download=True)\n",
    "    cifar_test = DataLoader(cifar_test, batch_size=batchsz, shuffle=True)\n",
    "\n",
    "    x, label = iter(cifar_train).next()\n",
    "    print('x:', x.shape, 'label:', label.shape)\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    # model = Lenet5().to(device)\n",
    "    model = ResNet18().to(device)\n",
    "    # logits => Softmax => pred\n",
    "    criteon = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "    print(model)\n",
    "\n",
    "    for epoch in range(1000):\n",
    "\n",
    "        model.train()\n",
    "        for batchidx, (x, label) in enumerate(cifar_train):\n",
    "            # [b, 3, 32, 32]\n",
    "            # [b]\n",
    "            x, label = x.to(device), label.to(device)\n",
    "\n",
    "            logits = model(x)\n",
    "            # logits: [b, 10]\n",
    "            # label: [b] here doesn't need one-hot and probability\n",
    "            # loss: tensor scalar\n",
    "            loss = criteon(logits, label)\n",
    "\n",
    "            # backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        #\n",
    "        print(epoch, 'loss:', loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # test\n",
    "            total_correct = 0\n",
    "            total_num = 0\n",
    "            for x, label in cifar_test:\n",
    "                # [b, 3, 32, 32]\n",
    "                # [b]\n",
    "                x, label = x.to(device), label.to(device)\n",
    "\n",
    "                # [b, 10]\n",
    "                logits = model(x)\n",
    "                # [b]\n",
    "                pred = logits.argmax(dim=1)\n",
    "                # [b] vs [b] => scalar tensor\n",
    "                total_correct += torch.eq(pred, label).float().sum().item()\n",
    "                total_num += x.size(0)\n",
    "\n",
    "            acc = total_correct / total_num\n",
    "            print(epoch, acc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
